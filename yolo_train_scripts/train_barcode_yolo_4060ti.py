#!/usr/bin/env python
"""
YOLOæ¡ç æ£€æµ‹æ¨¡åž‹GPUè®­ç»ƒè„šæœ¬ - é’ˆå¯¹4060tiæ˜¾å¡ä¼˜åŒ–
å……åˆ†åˆ©ç”¨4060tiçš„8GBæ˜¾å­˜å’ŒCUDAåŠ é€Ÿèƒ½åŠ›
"""
from ultralytics import YOLO
import os
import torch
import time
import psutil
from datetime import datetime

def check_gpu_environment():
    """æ£€æŸ¥GPUçŽ¯å¢ƒå’Œç³»ç»Ÿèµ„æº"""
    print("=== ç³»ç»ŸçŽ¯å¢ƒæ£€æŸ¥ ===")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ“ CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print(f"âœ“ GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"âœ“ æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"âœ“ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}")
        return True
    else:
        print("âœ— CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        return False

def get_optimal_batch_size():
    """æ ¹æ®4060tiæ˜¾å­˜åŠ¨æ€è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        # 4060tiæœ‰8GBæ˜¾å­˜ï¼Œä¿å®ˆä¼°è®¡ä½¿ç”¨70%æ˜¾å­˜ç”¨äºŽè®­ç»ƒ
        usable_memory = gpu_memory * 0.7
        
        # æ ¹æ®ç»éªŒå…¬å¼ï¼šæ¯GBæ˜¾å­˜å¯ä»¥å¤„ç†çº¦2-3ä¸ªæ ·æœ¬ï¼ˆ1024x1024ï¼‰
        if usable_memory >= 5.6:
            return 16
        elif usable_memory >= 4.0:
            return 12
        elif usable_memory >= 3.0:
            return 8
        else:
            return 4
    else:
        return 4  # CPUè®­ç»ƒä½¿ç”¨å°æ‰¹æ¬¡

def train_barcode_model_gpu():
    """ä½¿ç”¨GPUè®­ç»ƒæ¡ç æ£€æµ‹æ¨¡åž‹ - 4060tiä¼˜åŒ–ç‰ˆ"""
    
    # çŽ¯å¢ƒæ£€æŸ¥
    gpu_available = check_gpu_environment()
    device = '0' if gpu_available else 'cpu'
    
    # è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°
    optimal_batch_size = get_optimal_batch_size()
    
    print(f"\n=== è®­ç»ƒé…ç½® ===")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
    
    # è®­ç»ƒé…ç½® - é’ˆå¯¹4060tiä¼˜åŒ–
    config = {
        'model': 'yolov8n.pt',                    # ä½¿ç”¨nanoç‰ˆæœ¬ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
        'data': 'barcode_dataset/dataset.yaml',   # æœ¬åœ°ç›¸å¯¹è·¯å¾„
        'epochs': 50,                              # å‡å°‘è½®æ¬¡ä»¥é™ä½Žå†…å­˜åŽ‹åŠ›
        'batch_size': 4,                           # å›ºå®šå°æ‰¹æ¬¡é¿å…å†…å­˜é—®é¢˜
        'img_size': 640,                           # å‡å°å›¾åƒå°ºå¯¸é™ä½Žå†…å­˜ä½¿ç”¨
        'device': device,                          # ä½¿ç”¨GPU
        'name': 'barcode_detector_4060ti',        # æ–°åç§°é¿å…å†²çª
        'project': 'barcode_training',            # é¡¹ç›®åç§°
        'save_period': 10,                        # æ¯10è½®ä¿å­˜ä¸€æ¬¡
        'patience': 30,                           # æ—©åœè€å¿ƒå€¼
        'verbose': True,
        'plots': True,                           # ç”Ÿæˆè®­ç»ƒå›¾è¡¨
        'save_json': True,                       # ä¿å­˜JSONç»“æžœ
        'exist_ok': True,                        # å…è®¸è¦†ç›–çŽ°æœ‰é¡¹ç›®
        'workers': 2,                            # å‡å°‘çº¿ç¨‹æ•°é™ä½Žå†…å­˜ä½¿ç”¨
        'cache': False,                          # ç¦ç”¨ç¼“å­˜å‡å°‘å†…å­˜åŽ‹åŠ›
        'optimizer': 'AdamW',                    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
        'lr0': 0.001,                            # åˆå§‹å­¦ä¹ çŽ‡
        'conf': 0.25,                           # éªŒè¯ç½®ä¿¡åº¦é˜ˆå€¼
        'rect': False,                           # ç¦ç”¨çŸ©å½¢è®­ç»ƒ
        'cos_lr': True,                          # ä½¿ç”¨ä½™å¼¦å­¦ä¹ çŽ‡è°ƒåº¦
        'close_mosaic': 10,                      # æœ€åŽ10è½®å…³é—­mosaicå¢žå¼º
        'amp': False,                            # ç¦ç”¨æ··åˆç²¾åº¦å‡å°‘å†…å­˜é—®é¢˜
        'multi_scale': False,                     # ç¦ç”¨å¤šå°ºåº¦è®­ç»ƒé™ä½Žå†…å­˜
        'fraction': 1.0,                        # ä½¿ç”¨å…¨éƒ¨æ•°æ®
        'profile': True,                         # åˆ†æžè®­ç»ƒæ€§èƒ½
        'freeze': None,                          # ä¸å†»ç»“å±‚
        'warmup_epochs': 3.0,                    # é¢„çƒ­è½®æ¬¡
        'warmup_momentum': 0.8,                  # é¢„çƒ­åŠ¨é‡
        'warmup_bias_lr': 0.1,                   # é¢„çƒ­åç½®å­¦ä¹ çŽ‡
        'box': 7.5,                              # æ¡†æŸå¤±æƒé‡
        'cls': 0.5,                              # ç±»åˆ«æŸå¤±æƒé‡
        'dfl': 1.5,                              # åˆ†å¸ƒç„¦ç‚¹æŸå¤±æƒé‡
        # æ•°æ®å¢žå¼ºå‚æ•°
        'hsv_h': 0.015,                          # è‰²è°ƒå¢žå¼º
        'hsv_s': 0.7,                            # é¥±å’Œåº¦å¢žå¼º
        'hsv_v': 0.4,                            # æ˜Žåº¦å¢žå¼º
        'degrees': 0.0,                          # æ—‹è½¬è§’åº¦ï¼ˆæ¡ç é€šå¸¸ä¸éœ€è¦æ—‹è½¬ï¼‰
        'translate': 0.1,                         # å¹³ç§»
        'scale': 0.5,                            # ç¼©æ”¾
        'shear': 0.0,                            # å‰ªåˆ‡
        'perspective': 0.0,                      # é€è§†
        'flipud': 0.0,                           # ä¸Šä¸‹ç¿»è½¬æ¦‚çŽ‡
        'fliplr': 0.5,                           # å·¦å³ç¿»è½¬æ¦‚çŽ‡
        'mosaic': 1.0,                           # Mosaicå¢žå¼º
        'mixup': 0.0,                            # MixUpå¢žå¼º
        'copy_paste': 0.0,                       # å¤åˆ¶ç²˜è´´å¢žå¼º
    }
    
    print(f"è®­ç»ƒé…ç½®å‚æ•°: {config}")
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config['data']):
        print(f"é”™è¯¯: æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config['data']}")
        print("è¯·ç¡®ä¿barcode_dataset/dataset.yamlæ–‡ä»¶å­˜åœ¨ä¸”é…ç½®æ­£ç¡®")
        return None
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡åž‹æ–‡ä»¶
    if not os.path.exists(config['model']):
        print(f"é¢„è®­ç»ƒæ¨¡åž‹ä¸å­˜åœ¨: {config['model']}")
        print("å°†è‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹...")
    
    # æ˜¾ç¤ºè®­ç»ƒå‰çš„å†…å­˜å’Œæ˜¾å­˜çŠ¶æ€
    if gpu_available:
        torch.cuda.empty_cache()
        print(f"è®­ç»ƒå‰æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    print(f"å¼€å§‹GPUè®­ç»ƒæ¡ç æ£€æµ‹æ¨¡åž‹...")
    print(f"é¢„è®¡è®­ç»ƒæ—¶é—´: 2-3å°æ—¶ï¼ˆ100è½®ï¼‰")
    
    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        # åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
        print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹: {config['model']}")
        model = YOLO(config['model'])
        
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹GPUè®­ç»ƒ...")
        results = model.train(
            data=config['data'],
            epochs=config['epochs'],
            batch=config['batch_size'],
            imgsz=config['img_size'],
            device=config['device'],
            name=config['name'],
            project=config['project'],
            save_period=config['save_period'],
            patience=config['patience'],
            verbose=config['verbose'],
            plots=config['plots'],
            save_json=config['save_json'],
            exist_ok=config['exist_ok'],
            workers=config['workers'],
            cache=config['cache'],
            optimizer=config['optimizer'],
            lr0=config['lr0'],
            conf=config['conf'],
            rect=config['rect'],
            cos_lr=config['cos_lr'],
            close_mosaic=config['close_mosaic'],
            amp=config['amp'],
            multi_scale=config['multi_scale'],
            fraction=config['fraction'],
            profile=config['profile'],
            freeze=config['freeze'],
            warmup_epochs=config['warmup_epochs'],
            warmup_momentum=config['warmup_momentum'],
            warmup_bias_lr=config['warmup_bias_lr'],
            box=config['box'],
            cls=config['cls'],
            dfl=config['dfl'],
            hsv_h=config['hsv_h'],
            hsv_s=config['hsv_s'],
            hsv_v=config['hsv_v'],
            degrees=config['degrees'],
            translate=config['translate'],
            scale=config['scale'],
            shear=config['shear'],
            perspective=config['perspective'],
            flipud=config['flipud'],
            fliplr=config['fliplr'],
            mosaic=config['mosaic'],
            mixup=config['mixup'],
            copy_paste=config['copy_paste'],
        )
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        end_time = time.time()
        training_time = end_time - start_time
        
        print("\n" + "="*50)
        print("è®­ç»ƒå®Œæˆ!")
        print(f"è®­ç»ƒç”¨æ—¶: {training_time/3600:.2f} å°æ—¶")
        print(f"æœ€ä½³æ¨¡åž‹ä¿å­˜åœ¨: {config['project']}/{config['name']}/weights/best.pt")
        print(f"æœ€ç»ˆæ¨¡åž‹ä¿å­˜åœ¨: {config['project']}/{config['name']}/weights/last.pt")
        
        # æ˜¾ç¤ºè®­ç»ƒåŽçš„å†…å­˜çŠ¶æ€
        if gpu_available:
            print(f"è®­ç»ƒåŽæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        return results
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("å¯èƒ½çš„åŽŸå› :")
        print("1. æ˜¾å­˜ä¸è¶³ - å°è¯•å‡å°æ‰¹æ¬¡å¤§å°")
        print("2. æ•°æ®é›†è·¯å¾„é”™è¯¯ - æ£€æŸ¥dataset.yamlé…ç½®")
        print("3. CUDAé©±åŠ¨é—®é¢˜ - æ£€æŸ¥GPUé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        return None
    finally:
        # æ¸…ç†æ˜¾å­˜
        if gpu_available:
            torch.cuda.empty_cache()

def validate_model(model_path, data_path, device='0'):
    """éªŒè¯æ¨¡åž‹æ€§èƒ½"""
    print(f"\n=== æ¨¡åž‹éªŒè¯ ===")
    print(f"éªŒè¯æ¨¡åž‹: {model_path}")
    print(f"æ•°æ®é›†: {data_path}")
    
    try:
        model = YOLO(model_path)
        
        # è¿è¡ŒéªŒè¯
        print("å¼€å§‹éªŒè¯...")
        metrics = model.val(data=data_path, device=device)
        
        print("\néªŒè¯ç»“æžœ:")
        print(f"mAP50: {metrics.box.map50:.4f}")
        print(f"mAP50-95: {metrics.box.map:.4f}")
        print(f"ç²¾åº¦: {metrics.box.mp:.4f}")
        print(f"å¬å›žçŽ‡: {metrics.box.mr:.4f}")
        
        # æ€§èƒ½è¯„ä¼°
        if metrics.box.map50 >= 0.9:
            print("âœ“ æ¨¡åž‹æ€§èƒ½ä¼˜ç§€ (mAP50 >= 0.9)")
        elif metrics.box.map50 >= 0.8:
            print("âœ“ æ¨¡åž‹æ€§èƒ½è‰¯å¥½ (mAP50 >= 0.8)")
        elif metrics.box.map50 >= 0.7:
            print("â–³ æ¨¡åž‹æ€§èƒ½ä¸€èˆ¬ (mAP50 >= 0.7)")
        else:
            print("âœ— æ¨¡åž‹æ€§èƒ½è¾ƒå·® (mAP50 < 0.7)")
        
        return metrics
        
    except Exception as e:
        print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None

def test_model_inference_speed(model_path, test_image_count=10):
    """æµ‹è¯•æ¨¡åž‹æŽ¨ç†é€Ÿåº¦"""
    print(f"\n=== æŽ¨ç†é€Ÿåº¦æµ‹è¯• ===")
    
    try:
        import cv2
        import numpy as np
        
        model = YOLO(model_path)
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_images = []
        for i in range(test_image_count):
            # ç”Ÿæˆéšæœºæµ‹è¯•å›¾åƒ (1024x1024)
            test_img = np.random.randint(0, 255, (1024, 1024, 3), dtype=np.uint8)
            test_images.append(test_img)
        
        # é¢„çƒ­GPU
        if torch.cuda.is_available():
            _ = model(test_images[0], device='0')
        
        # æµ‹è¯•æŽ¨ç†é€Ÿåº¦
        start_time = time.time()
        for img in test_images:
            _ = model(img, device='0')
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time = total_time / test_image_count
        fps = 1 / avg_time
        
        print(f"æµ‹è¯•å›¾åƒæ•°é‡: {test_image_count}")
        print(f"æ€»æŽ¨ç†æ—¶é—´: {total_time:.2f} ç§’")
        print(f"å¹³å‡æŽ¨ç†æ—¶é—´: {avg_time*1000:.2f} æ¯«ç§’")
        print(f"æŽ¨ç†é€Ÿåº¦: {fps:.2f} FPS")
        
        return {
            'total_time': total_time,
            'avg_time_ms': avg_time * 1000,
            'fps': fps
        }
        
    except Exception as e:
        print(f"é€Ÿåº¦æµ‹è¯•å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    print("YOLOæ¡ç æ£€æµ‹GPUè®­ç»ƒè„šæœ¬ - 4060tiä¼˜åŒ–ç‰ˆ")
    print("="*60)
    
    # è®­ç»ƒæ¨¡åž‹
    results = train_barcode_model_gpu()
    
    if results:
        # éªŒè¯æœ€ä½³æ¨¡åž‹
        best_model_path = 'barcode_training/barcode_detector_4060ti/weights/best.pt'
        data_path = 'barcode_dataset/dataset.yaml'
        device = '0' if torch.cuda.is_available() else 'cpu'
        
        # è¿è¡ŒéªŒè¯
        metrics = validate_model(best_model_path, data_path, device)
        
        # æµ‹è¯•æŽ¨ç†é€Ÿåº¦
        speed_results = test_model_inference_speed(best_model_path)
        
        if metrics and speed_results:
            print("\n" + "="*60)
            print("ðŸŽ‰ è®­ç»ƒå’Œæµ‹è¯•å…¨éƒ¨å®Œæˆ!")
            print(f"âœ“ æ¨¡åž‹æ€§èƒ½: mAP50-95 = {metrics.box.map:.4f}")
            print(f"âœ“ æŽ¨ç†é€Ÿåº¦: {speed_results['fps']:.2f} FPS")
            print(f"âœ“ æ¨¡åž‹è·¯å¾„: {best_model_path}")
            print("="*60)
        else:
            print("\nâš ï¸ è®­ç»ƒå®Œæˆï¼Œä½†éªŒè¯æˆ–é€Ÿåº¦æµ‹è¯•å¤±è´¥")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        print("è¯·æ£€æŸ¥:")
        print("1. GPUçŽ¯å¢ƒé…ç½®")
        print("2. æ•°æ®é›†è·¯å¾„")
        print("3. ä¾èµ–åº“å®‰è£…")